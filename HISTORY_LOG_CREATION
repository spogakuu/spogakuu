from pyspark.sql.types import StructType, StructField, StringType, DateType, IntegerType
from pyspark.sql import SparkSession
import os
from datetime import datetime

# Initialize Spark session
spark = SparkSession.builder.getOrCreate()

# Define updated schema for daily log (timestamp columns as strings)
daily_log_schema = StructType([
    StructField("table_name", StringType(), True),
    StructField("status", StringType(), True),
    StructField("load_date", StringType(), True),        # CST run date (as string)
    StructField("UTCTimestamp", StringType(), True),     # "2024-06-24T00:00:00Z"
    StructField("start_time", StringType(), True),       # ISO timestamp string
    StructField("end_time", StringType(), True),         # ISO timestamp string
    StructField("count", IntegerType(), True),           # Row count
    StructField("error_message", StringType(), True)     # Error info if failed
])

# Get today's date string for path naming
today_str = datetime.now().strftime("%Y-%m-%d")

# Path where daily log will be stored
daily_log_path = f"Files/BRONZE/HISTORY/LOGS/Daily_History_Logs"

# Use Fabric-compatible fs check (instead of os.path.exists)
from notebookutils import mssparkutils
if not mssparkutils.fs.exists(daily_log_path):
    empty_df = spark.createDataFrame([], daily_log_schema)
    empty_df.write.mode("overwrite").parquet(daily_log_path)
    print(f"✅ Created empty daily log: {daily_log_path}")
else:
    print(f"ℹ️ Daily log already exists: {daily_lo_
