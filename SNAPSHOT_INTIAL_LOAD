from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sha2, concat_ws, when, lit

# -------------------------
# CHANGE THESE ONLY
# -------------------------
SOURCE_TABLE = "BRONZE.INITIAL_LOAD.YOUR_TABLE_NAME"
TARGET_TABLE = "BRONZE.SNAPSHOT_INCREMENTAL.YOUR_TABLE_NAME"
TEMP_TABLE = TARGET_TABLE + "_TEMP"
# -------------------------

# Initialize Spark
spark = SparkSession.builder.appName("Initial Load Snapshot").getOrCreate()
spark.conf.set("spark.sql.session.timeZone", "UTC")

# Function to cast utctimestamp to string and add ROW_HASH
def prepare_snapshot(df):
    # Convert utctimestamp to string (permanent change in schema)
    if "utctimestamp" in df.columns:
        df = df.withColumn("utctimestamp", col("utctimestamp").cast("string"))

    # Create cleaned version of all fields for hashing
    cleaned_cols = []
    for field in df.schema.fields:
        col_name = field.name
        cleaned_col = when(col(col_name).isNull(), lit(" ")).otherwise(col(col_name).cast("string"))
        cleaned_cols.append(cleaned_col.alias(col_name))

    temp_df = df.select(*cleaned_cols)
    row_hash_col = sha2(concat_ws("||", *temp_df.columns), 256).alias("ROW_HASH")

    # Add ROW_HASH to actual df
    return df.withColumn("ROW_HASH", row_hash_col)

# Load → transform → write temp → rename to target
try:
    print(f"\nSTART processing: {SOURCE_TABLE}")

    # Read source
    df = spark.read.table(SOURCE_TABLE)

    # Transform (utctimestamp as string + add ROW_HASH)
    df_final = prepare_snapshot(df).coalesce(1)

    # Drop temp table if exists
    spark.sql(f"DROP TABLE IF EXISTS {TEMP_TABLE}")

    # Write to temp table
    df_final.write.mode("overwrite").saveAsTable(TEMP_TABLE)

    # Replace target table
    spark.sql(f"DROP TABLE IF EXISTS {TARGET_TABLE}")
    spark.sql(f"ALTER TABLE {TEMP_TABLE} RENAME TO {TARGET_TABLE}")

    print(f"SUCCESS: {TARGET_TABLE} created with ROW_HASH and utctimestamp as string")

except Exception as e:
    print(f"FAILED: {e}")
