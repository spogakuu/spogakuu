from pyspark.sql.functions import col, split, lit
from datetime import datetime
import pytz
import json
import os
from notebookutils import mssparkutils

# === CONFIG ===
log_path = "Files/BRONZE/STAGING_LOGS"
table_list_path = "Files/TABLES_LIST"

# === TIMEZONE ===
tz = pytz.timezone("America/Chicago")
today = datetime.now(tz).date()

# === Get Table and UTC from base parameters ===
table_name = mssparkutils.env.get("table_name")
utc_timestamp = mssparkutils.env.get("utc_timestamp")

# === Load active tables ===
df_all = spark.read.parquet(table_list_path)
df_active = df_all.filter((col("STATUS") == "A") & (col("LOAD_TYPE") == "INCREMENTAL"))

try:
    # === Read staging logs ===
    df_log = spark.read.parquet(log_path)

    # === Filter for today, table, and UTC ===
    df_filtered = df_log.filter(
        (col("UTCTimestamp") == utc_timestamp) &
        (col("table_name").startswith(table_name))  # e.g., patient_data_INSERT
    )

    if df_filtered.count() == 0:
        # No entry â†’ return full operations for this table
        meta = df_active.filter(col("TABLE_NAME") == table_name).collect()[0]
        result = [
            {
                "table_name": meta["TABLE_NAME"],
                "key_column": meta["KEY_COLUMN"],
                "status": meta["STATUS"],
                "database": meta["DATABASE"],
                "schema": meta["SCHEMA"],
                "operation": "ALL"
            }
        ]
        mssparkutils.notebook.exit(json.dumps(result))

    # === Split by operation & filter by status ===
    df_failed = df_filtered.filter(col("status") == "Failed")
    df_succeeded = df_filtered.filter(col("status") == "Succeeded")

    operations_to_retry = df_failed.select(split(col("table_name"), "_").getItem(-1).alias("operation")) \
                                   .distinct() \
                                   .rdd.flatMap(lambda x: x).collect()

    if not operations_to_retry:
        # All operations succeeded for this table & UTC
        mssparkutils.notebook.exit("EXIT")

    # === Prepare result ===
    meta = df_active.filter(col("TABLE_NAME") == table_name).collect()[0]
    result = [
        {
            "table_name": meta["TABLE_NAME"],
           
