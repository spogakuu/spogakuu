from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sha2, concat_ws, when, lit, date_format, to_date
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timezone
import pytz
import threading
import traceback
import os
import uuid
from notebookutils import mssparkutils

# === INIT SPARK SESSION ===
spark = SparkSession.builder.appName("Process Incremental Tables").getOrCreate()
spark.conf.set("spark.sql.session.timeZone", "UTC")
spark.conf.set("spark.sql.parquet.int96RebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInWrite", "CORRECTED")

# === TIMEZONE ===
tz = pytz.timezone("America/Chicago")
Today_date = datetime.now(tz).strftime("%Y-%m-%d")

# === PATH CONFIG ===
staging_path = "Files/BRONZE/INCREMENTAL_LOAD"
copy_path = "Files/BRONZE/HISTORY/FULL_HISTORY"
table_list_path = "Files/TABLES_LIST"
daily_log_path = "Files/BRONZE/HISTORY/LOGS/Daily_History_Logs"
main_log_path = "Files/BRONZE/HISTORY/LOGS/Final_History_Logs"
lock_file = "/tmp/update_log.lock"

def acquire_lock():
    if os.path.exists(lock_file): return False
    with open(lock_file, 'w') as f: f.write("locked")
    return True

def release_lock():
    if os.path.exists(lock_file): os.remove(lock_file)

def safe_read_parquet(path):
    if not mssparkutils.fs.exists(path):
        print(f"[SKIP] File not found: {path}")
        return None
    try:
        return spark.read.parquet(path)
    except Exception as e:
        print(f"[ERROR] Failed to read {path}: {str(e)}")
        return None

def add_hash_column(df):
    exclude_cols = {"ROW_HASH", "insert_timestamp", "flag", "UTCTimestamp"}
    cleaned_cols = [
        when(col(field.name).isNull(), lit(" ")).otherwise(col(field.name).cast("string")).alias(field.name)
        for field in df.schema.fields if field.name not in exclude_cols
    ]
    temp_df = df.select(*cleaned_cols)
    row_hash_col = sha2(concat_ws("||", *temp_df.columns), 256).alias("ROW_HASH")
    return df.withColumn("ROW_HASH", row_hash_col)

def update_log(table_name, status, utc_timestamp_str, start_time, end_time, count=0, error_message=""):
    try:
        utc_date = datetime.strptime(utc_timestamp_str, "%Y-%m-%d").date()
        utc_ts_full = datetime.combine(utc_date, datetime.min.time()).replace(tzinfo=timezone.utc)
        run_date = datetime.now(pytz.timezone("America/Chicago")).date()
        new_row = [(table_name, status, run_date, utc_ts_full, start_time.isoformat(), end_time.isoformat(), count, error_message)]

        try:
            schema = spark.read.parquet(daily_log_path).schema
        except:
            schema = spark.read.parquet(main_log_path).schema
            spark.createDataFrame([], schema).write.mode("overwrite").parquet(daily_log_path)

        new_df = spark.createDataFrame(new_row, schema)

        while not acquire_lock(): continue
        try:
            try:
                existing_df = spark.read.parquet(daily_log_path)
                filtered_df = existing_df.filter(~(
                    (col("table_name") == table_name) &
                    (col("load_date") == run_date) &
                    (to_date(col("UTCTimestamp")) == utc_date) &
                    (col("status") == status)
                ))
                final_df = filtered_df.unionByName(new_df)
            except:
                final_df = new_df

            temp_daily_path = f"{daily_log_path}_temp_{uuid.uuid4()}"
            final_df.write.mode("overwrite").parquet(temp_daily_path)
            fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
            Path = spark._jvm.org.apache.hadoop.fs.Path
            if fs.exists(Path(daily_log_path)): fs.delete(Path(daily_log_path), True)
            fs.rename(Path(temp_daily_path), Path(daily_log_path))
        finally:
            release_lock()
    except Exception as e:
        print(f"[LOG ERROR] Failed to update log for {table_name} | {status} | {traceback.format_exc()}")

def get_unprocessed_utcs_for_table(table_name, log_df, mode="ALL"):
    insert_path = f"{staging_path}/{table_name}_INSERT"
    insert_df = safe_read_parquet(insert_path)
    if not insert_df:
        print(f"[SKIP] No insert data found for {table_name}")
        return []

    utc_dates_in_data = insert_df.select(date_format(col("UTCTimestamp"), "yyyy-MM-dd")).distinct().rdd.flatMap(lambda x: x).collect()

    if mode == "FAILURE":
        logged_utcs = log_df.filter((col("table_name") == table_name) & (col("status") == "FAILURE")) \
                            .select("UTCTimestamp").rdd.flatMap(lambda x: x) \
                            .map(lambda d: d.strftime("%Y-%m-%d")).distinct().collect()
        return [utc for utc in utc_dates_in_data if utc in logged_utcs]

    elif mode == "UNPROCESSED":
        logged_utcs = log_df.filter(col("table_name") == table_name) \
                            .select("UTCTimestamp").rdd.flatMap(lambda x: x) \
                            .map(lambda d: d.strftime("%Y-%m-%d")).distinct().collect()
        return [utc for utc in utc_dates_in_data if utc not in logged_utcs]

    else:
        logged_utcs = log_df.filter(col("table_name") == table_name) \
                            .select("UTCTimestamp").rdd.flatMap(lambda x: x) \
                            .map(lambda d: d.strftime("%Y-%m-%d")).distinct().collect()
        return [utc for utc in utc_dates_in_data if utc not in logged_utcs]

def load_table(table_name, key_column):
    thread_name = threading.current_thread().name
    start_time_total = datetime.now()
    try:
        copy_table_path = f"{copy_path}/{table_name}_HIST"
        temp_table_path = f"{copy_path}/{table_name}_HIST_TEMP"
        delete_storage_path = f"{copy_path}/{table_name}_HIST_DELETE"

        insert_df = spark.read.parquet(f"{staging_path}/{table_name}_INSERT")
        update_df = spark.read.parquet(f"{staging_path}/{table_name}_UPDATE")
        delete_df = spark.read.parquet(f"{staging_path}/{table_name}_DELETE")

        insert_utcs = insert_df.select(date_format(col("UTCTimestamp"), "yyyy-MM-dd").alias("utc_day")).distinct().rdd.flatMap(lambda x: x).collect()
        update_utcs = update_df.select(date_format(col("UTCTimestamp"), "yyyy-MM-dd").alias("utc_day")).distinct().rdd.flatMap(lambda x: x).collect()
        delete_utcs = delete_df.select(date_format(col("UTCTimestamp"), "yyyy-MM-dd").alias("utc_day")).distinct().rdd.flatMap(lambda x: x).collect()

        utc_days = sorted(set(insert_utcs + update_utcs + delete_utcs))
        if not utc_days:
            print(f"[SKIP] No UTCs found for {table_name}")
            return

        for utc_day in utc_days:
            try:
                copy_table = safe_read_parquet(copy_table_path)
                if copy_table is None:
                    copy_table = spark.createDataFrame([], insert_df.schema.add("flag", "string"))

                delete_view = delete_df.filter(date_format(col("UTCTimestamp"), "yyyy-MM-dd") == utc_day)
                if not delete_view.rdd.isEmpty():
                    active_history = copy_table.filter(col("flag") == "A")
                    delete_keys = delete_view.select(col("row_number").alias(key_column)).distinct()
                    records_to_inactivate = active_history.join(delete_keys, key_column, "inner").withColumn("flag", lit("I"))
                    records_to_inactivate = add_hash_column(records_to_inactivate)
                    copy_table = copy_table.unionByName(records_to_inactivate)
                    delete_view.write.mode("append").parquet(delete_storage_path)

                insert_view = insert_df.filter(date_format(col("UTCTimestamp"), "yyyy-MM-dd") == utc_day).withColumn("flag", lit("A"))
                update_view = update_df.filter(date_format(col("UTCTimestamp"), "yyyy-MM-dd") == utc_day).withColumn("flag", lit("A"))
                merged_view = insert_view.unionByName(update_view, allowMissingColumns=True)

                if not merged_view.rdd.isEmpty():
                    copy_table = copy_table.unionByName(merged_view, allowMissingColumns=True)
                    copy_table = add_hash_column(copy_table)

                fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
                Path = spark._jvm.org.apache.hadoop.fs.Path
                temp_path = Path(temp_table_path)
                final_path = Path(copy_table_path)
                if fs.exists(temp_path): fs.delete(temp_path, True)
                copy_table.write.mode("overwrite").parquet(temp_table_path)
                if fs.exists(final_path): fs.delete(final_path, True)
                fs.rename(temp_path, final_path)

                end_time = datetime.now()
                update_log(table_name, "D", utc_day, start_time_total, end_time, delete_view.count())
                update_log(table_name, "I", utc_day, start_time_total, end_time, insert_view.count())
                update_log(table_name, "U", utc_day, start_time_total, end_time, update_view.count())
            except Exception as e:
                update_log(table_name, "FAILURE", utc_day, start_time_total, datetime.now(), 0, traceback.format_exc())
    except Exception as e:
        update_log(table_name, "FAILURE", Today_date, start_time_total, datetime.now(), 0, traceback.format_exc())

# === Load Active Tables ===
table_list_df = spark.read.parquet(table_list_path)
active_tables_df = table_list_df.filter("STATUS = 'A'")
active_tables = [(row["TABLE_NAME"], row["KEY_COLUMN"]) for row in active_tables_df.collect()]

# === Determine tables to run based on failed or unprocessed UTCs ===
tables_to_run = []
try:
    main_log_df = spark.read.parquet(main_log_path)
    failed_today_df = main_log_df.filter((col("load_date") == Today_date) & (col("status") == "FAILURE"))
    failed_today_tables = [row["table_name"] for row in failed_today_df.collect()]

    if failed_today_tables:
        print("[MODE] Retrying tables with failed UTCs only")
        for table_name, key_column in active_tables:
            if table_name in failed_today_tables:
                utc_days = get_unprocessed_utcs_for_table(table_name, main_log_df, mode="FAILURE")
                if utc_days:
                    tables_to_run.append((table_name, key_column))
    else:
        print("[MODE] Checking all active tables for unprocessed UTCs")
        for table_name, key_column in active_tables:
            utc_days = get_unprocessed_utcs_for_table(table_name, main_log_df, mode="UNPROCESSED")
            if utc_days:
                tables_to_run.append((table_name, key_column))
except:
    print("[ERROR] Failed to load main log; fallback to all tables")
    for table_name, key_column in active_tables:
        utc_days = get_unprocessed_utcs_for_table(table_name, spark.createDataFrame([], schema=None))
        if utc_days:
            tables_to_run.append((table_name, key_column))

# === Run Tables in Thread Pool ===
if tables_to_run:
    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = {executor.submit(load_table, table, key): table for table, key in tables_to_run}
        for future in as_completed(futures):
            print(f"[THREAD COMPLETE] Table: {futures[future]}")

# === FINAL LOG MERGE WITH SAFE OVERWRITE ===
try:
    main_log_df = spark.read.parquet(main_log_path)
    daily_log_df = spark.read.parquet(daily_log_path)
except:
    print("[MERGE] Daily log not found or empty.")
    daily_log_df = spark.createDataFrame([], main_log_df.schema)

join_keys = ["table_name", "load_date", "UTCTimestamp", "status"]
cleaned_main_log_df = main_log_df.alias("main").join(
    daily_log_df.select(*join_keys).alias("daily"),
    on=join_keys,
    how="left_anti"
)
final_main_log_df = cleaned_main_log_df.unionByName(daily_log_df)

temp_main_log_path = main_log_path + "_tmp"
final_main_log_df.write.mode("overwrite").parquet(temp_main_log_path)

fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
Path = spark._jvm.org.apache.hadoop.fs.Path
if fs.exists(Path(main_log_path)): fs.delete(Path(main_log_path), True)
fs.rename(Path(temp_main_log_path), Path(main_log_path))
if fs.exists(Path(daily_log_path)): fs.delete(Path(daily_log_path), True)

print("[COMPLETE] Final log updated and daily log cleaned.")
