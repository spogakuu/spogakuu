from pyspark.sql import SparkSession, functions as F
from datetime import datetime
import pytz, traceback
from concurrent.futures import ThreadPoolExecutor, as_completed

# === INIT SPARK SESSION ===
spark = SparkSession.builder.appName("Process History Tables").getOrCreate()
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
spark.conf.set("spark.sql.session.timeZone", "UTC")
spark.conf.set("spark.sql.parquet.int96RebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInWrite", "CORRECTED")

# === CONFIG ===
tz = pytz.timezone("America/Chicago")
Today_date = datetime.now(tz).date()

def _ts():
    return datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")

# === LOGGING FUNCTIONS ===
def begin_daily_log(table_name):
    print(f"[{_ts()}] === START TABLE: {table_name} ===")
    log_df = spark.createDataFrame([
        (Today_date, table_name, "Running", 0, 0, 0, None, datetime.now(tz))
    ], ["run_date","table_name","status","inserts","updates","deletes","error_message","insert_timestamp"])
    log_df.write.mode("append").saveAsTable("logs.daily_log")

def complete_daily_log(table_name, status, inserts=0, updates=0, deletes=0, error_message=None):
    print(f"[{_ts()}] === END TABLE: {table_name} | STATUS: {status} | I={inserts} U={updates} D={deletes} ===")
    upd_df = spark.createDataFrame([
        (Today_date, table_name, status, inserts, updates, deletes, error_message, datetime.now(tz))
    ], ["run_date","table_name","status","inserts","updates","deletes","error_message","insert_timestamp"])
    upd_df.write.mode("append").saveAsTable("logs.daily_log")

def get_today_status(table_name):
    try:
        df = spark.read.table("logs.daily_log").filter(
            (F.col("run_date") == F.lit(Today_date)) &
            (F.col("table_name") == F.lit(table_name))
        ).orderBy(F.col("insert_timestamp").desc())
        rows = df.limit(1).collect()
        return rows[0]["status"] if rows else None
    except:
        return None

# === UTILS ===
def safe_load_prev_raw_table(table_name):
    try:
        return spark.read.table(f"MHMR_LAKEHOUSE.PREV_RAW.{table_name}_RAW_PREV")
    except:
        return None

def move_raw_to_prev_raw(table_name):
    print(f"[{_ts()}] Moving RAW -> PREV_RAW for {table_name}")
    raw_df = spark.read.table(f"MHMR_LAKEHOUSE.RAW.{table_name}_RAW")
    raw_df.write.mode("overwrite").saveAsTable(f"MHMR_LAKEHOUSE.PREV_RAW.{table_name}_RAW_PREV")

def snapshot_prev_raw_table(table_name):
    snap_tbl = f"MHMR_LAKEHOUSE.PREV_RAW_SNAPSHOT.{table_name}_RAW_PREV_{Today_date.strftime('%Y%m%d')}"
    print(f"[{_ts()}] Snapshotting PREV_RAW -> {snap_tbl}")
    prev_df = spark.read.table(f"MHMR_LAKEHOUSE.PREV_RAW.{table_name}_RAW_PREV")
    prev_df.write.mode("overwrite").saveAsTable(snap_tbl)

# === CHANGE DETECTION ===
def build_changes(raw_df, prev_df, key_columns, row_hash_col, table_name):
    print(f"[{_ts()}] Detecting changes for {table_name} (Order: Deletes → Updates → Inserts)")
    join_keys = [F.col(f"raw.{k}") == F.col(f"prev.{k}") for k in key_columns]

    # Deletes
    deletes = (
        prev_df.alias("prev")
        .join(raw_df.alias("raw"), on=join_keys, how="left_anti")
        .select("prev.*")
        .withColumn("status", F.lit("D"))
        .withColumn("insert_ts", F.current_timestamp())
        .withColumn("source_table", F.lit(table_name))
    )

    # Updates
    updates = (
        raw_df.alias("raw")
        .join(prev_df.alias("prev"), on=join_keys, how="inner")
        .filter(F.col(f"raw.{row_hash_col}") != F.col(f"prev.{row_hash_col}"))
        .select("raw.*")
        .withColumn("status", F.lit("U"))
        .withColumn("insert_ts", F.current_timestamp())
        .withColumn("source_table", F.lit(table_name))
    )

    # Inserts
    inserts = (
        raw_df.alias("raw")
        .join(prev_df.alias("prev"), on=join_keys, how="left")
        .filter(F.col(f"prev.{key_columns[0]}").isNull())
        .select("raw.*")
        .withColumn("status", F.lit("I"))
        .withColumn("insert_ts", F.current_timestamp())
        .withColumn("source_table", F.lit(table_name))
    )

    cnts = {"D": deletes.count(), "U": updates.count(), "I": inserts.count()}
    print(f"[{_ts()}] Change counts: D={cnts['D']} | U={cnts['U']} | I={cnts['I']}")

    if cnts["D"] + cnts["U"] + cnts["I"] == 0:
        return None, cnts

    # Union in correct order
    changes_df = deletes.unionByName(updates).unionByName(inserts)
    return changes_df, cnts

# === WRITE STAGING ===
def write_to_temp_staging(df, table_name):
    temp_tbl = f"staging.temp_stg_{table_name}"
    print(f"[{_ts()}] Writing TEMP staging table: {temp_tbl}")
    df.write.mode("overwrite").saveAsTable(temp_tbl)

def finalize_staging_table(table_name):
    temp_tbl = f"staging.temp_stg_{table_name}"
    final_tbl = f"staging.stg_{table_name}"
    print(f"[{_ts()}] Promoting TEMP -> FINAL staging table: {final_tbl}")
    spark.sql(f"CREATE OR REPLACE TABLE {final_tbl} AS SELECT * FROM {temp_tbl}")
    spark.sql(f"DROP TABLE IF EXISTS {temp_tbl}")

# === PROCESS ONE TABLE ===
def process_single_table(row):
    table_name = row["TABLE_NAME"]
    key_columns = [k.strip() for k in row["KEY_COLUMN"].split(",")]
    row_hash_col = "row_hash"

    try:
        status = get_today_status(table_name)
        if status == "Succeeded":
            print(f"[{_ts()}] SKIP: {table_name} already processed successfully today.")
            return

        begin_daily_log(table_name)

        raw_df = spark.read.table(f"MHMR_LAKEHOUSE.RAW.{table_name}_RAW")
        prev_df = safe_load_prev_raw_table(table_name)

        if prev_df is None:
            cnt = raw_df.count()
            if cnt == 0:
                complete_daily_log(table_name, "Succeeded", 0, 0, 0)
                print(f"[{_ts()}] {table_name} - No data found for first run.")
                return
            changes_df = raw_df.withColumn("status", F.lit("I")) \
                .withColumn("insert_ts", F.current_timestamp()) \
                .withColumn("source_table", F.lit(table_name))
            counts = {"D": 0, "U": 0, "I": cnt}
        else:
            changes_df, counts = build_changes(raw_df, prev_df, key_columns, row_hash_col, table_name)
            if changes_df is None:
                complete_daily_log(table_name, "Succeeded", 0, 0, 0)
                print(f"[{_ts()}] {table_name} - No changes detected.")
                move_raw_to_prev_raw(table_name)
                snapshot_prev_raw_table(table_name)
                return

        # Write temp → Final only if changes exist
        write_to_temp_staging(changes_df, table_name)
        finalize_staging_table(table_name)

        # Move/snapshot
        move_raw_to_prev_raw(table_name)
        snapshot_prev_raw_table(table_name)

        complete_daily_log(table_name, "Succeeded", counts["I"], counts["U"], counts["D"])

    except Exception as e:
        print(f"[{_ts()}] ERROR: {table_name} failed - {e}")
        traceback.print_exc()
        complete_daily_log(table_name, "Failed", 0, 0, 0, error_message=str(e))

# === MAIN PARALLEL RUNNER ===
def process_all_tables():
    tables_list_df = spark.read.table("TABLES_LIST")
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(process_single_table, row.asDict()) for row in tables_list_df.collect()]
        for future in as_completed(futures):
            pass

# === RUN ===
process_all_tables()
