from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from functools import reduce
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import pytz, os, errno, time

# =========================
# INIT
# =========================
spark = SparkSession.builder.appName("Process History Tables").getOrCreate()
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
spark.conf.set("spark.sql.session.timeZone", "UTC")
spark.conf.set("spark.sql.parquet.int96RebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInWrite", "CORRECTED")

tz = pytz.timezone("America/Chicago")
Today_date = datetime.now(tz).date()
TODAY_STR = Today_date.strftime("%Y-%m-%d")

LOCK_PATH = "/tmp/staging_logs.lock"

DAILY_LOG = "staging.staging_daily_logs"
FINAL_LOG = "staging.staging_final_logs"

# =========================
# UTIL / LOCK
# =========================
def acquire_lock(path=LOCK_PATH, retries=60, delay=1.0):
    for _ in range(retries):
        try:
            fd = os.open(path, os.O_CREAT | os.O_EXCL | os.O_WRONLY)
            os.close(fd)
            return True
        except OSError as e:
            if e.errno != errno.EEXIST:
                raise
            time.sleep(delay)
    raise TimeoutError("Could not acquire log lock")

def release_lock(path=LOCK_PATH):
    try:
        os.remove(path)
    except FileNotFoundError:
        pass

def _and_all(conds):
    return reduce(lambda a, b: a & b, conds) if conds else F.lit(True)

def _select_with_alias(src_alias, cols):
    return [F.col(f"{src_alias}.{c}").alias(c) for c in cols]

def table_exists(tbl):
    try:
        spark.read.table(tbl).limit(1).count()
        return True
    except Exception:
        return False

def load_table_metadata():
    return spark.read.table("TABLES_LIST")

# =========================
# LOG TABLES
# =========================
def ensure_log_tables():
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {DAILY_LOG} (
            run_date DATE,
            table_name STRING,
            status STRING,
            inserts BIGINT,
            updates BIGINT,
            deletes BIGINT,
            attempt INT,
            error_message STRING,
            start_ts TIMESTAMP,
            end_ts TIMESTAMP,
            duration_seconds BIGINT
        ) USING DELTA
        PARTITIONED BY (run_date)
    """)
    spark.sql(f"""
        CREATE TABLE IF NOT EXISTS {FINAL_LOG} (
            run_date DATE,
            table_name STRING,
            status STRING,
            inserts BIGINT,
            updates BIGINT,
            deletes BIGINT,
            attempt INT,
            error_message STRING,
            start_ts TIMESTAMP,
            end_ts TIMESTAMP,
            duration_seconds BIGINT
        ) USING DELTA
        PARTITIONED BY (run_date)
    """)

# =========================
# DAILY LOG
# =========================
def begin_daily_log(table_name):
    acquire_lock()
    try:
        spark.sql(f"""
            MERGE INTO {DAILY_LOG} t
            USING (
                SELECT DATE('{TODAY_STR}') AS run_date,
                       '{table_name}'       AS table_name,
                       current_timestamp()  AS now_ts
            ) s
            ON t.run_date = s.run_date AND t.table_name = s.table_name
            WHEN MATCHED THEN UPDATE SET
                t.status        = 'Running',
                t.attempt       = COALESCE(t.attempt, 0) + 1,
                t.error_message = NULL,
                t.start_ts      = COALESCE(t.start_ts, s.now_ts),
                t.end_ts        = NULL,
                t.duration_seconds = NULL
            WHEN NOT MATCHED THEN INSERT (
                run_date, table_name, status, inserts, updates, deletes,
                attempt, error_message, start_ts, end_ts, duration_seconds
            ) VALUES (
                s.run_date, s.table_name, 'Running', NULL, NULL, NULL,
                1, NULL, s.now_ts, NULL, NULL
            )
        """)
    finally:
        release_lock()

def complete_daily_log(table_name, status, inserts=0, updates=0, deletes=0, error_message=None):
    acquire_lock()
    try:
        err_sql = "NULL" if not error_message else f"'{error_message.replace("'","''")}'"
        spark.sql(f"""
            UPDATE {DAILY_LOG}
            SET status = '{status}',
                inserts = {int(inserts)},
                updates = {int(updates)},
                deletes = {int(deletes)},
                error_message = {err_sql},
                end_ts = current_timestamp(),
                duration_seconds =
                    CASE
                        WHEN start_ts IS NOT NULL
                        THEN CAST(unix_timestamp(current_timestamp()) - unix_timestamp(start_ts) AS BIGINT)
                        ELSE NULL
                    END
            WHERE run_date = DATE('{TODAY_STR}') AND table_name = '{table_name}';
        """)
    finally:
        release_lock()

def get_today_status(table_name):
    try:
        df = spark.read.table(DAILY_LOG).where(
            (F.col("run_date") == F.lit(Today_date)) & (F.col("table_name") == F.lit(table_name))
        ).select("status").limit(1)
        rows = df.collect()
        return rows[0]["status"] if rows else None
    except Exception:
        return None

# =========================
# CHANGE DETECTION
# =========================
def safe_load_prev_raw_table(table_name):
    try:
        df = spark.read.table(f"MHMR_LAKEHOUSE.PREV_RAW.{table_name}_RAW_PREV")
        if df.limit(1).count() == 0:
            return None
        return df.alias("prev")
    except Exception:
        return None

def build_join_condition(df1_alias, df2_alias, key_columns):
    keys = [k.strip() for k in key_columns]
    return _and_all([F.col(f"{df1_alias}.{k}") == F.col(f"{df2_alias}.{k}") for k in keys])

def detect_inserts_updates(raw_df, raw_prev_df, key_columns, row_hash_col, table_name):
    on_cond = build_join_condition("raw", "prev", key_columns)
    joined = raw_df.alias("raw").join(raw_prev_df.alias("prev"), on=on_cond, how="left")
    comparison = (
        joined.withColumn(
            "status",
            F.when(F.col(f"prev.{row_hash_col}").isNull(), F.lit("I"))
             .when(F.col(f"raw.{row_hash_col}") != F.col(f"prev.{row_hash_col}"), F.lit("U"))
        ).filter(F.col("status").isNotNull())
    )
    raw_cols = raw_df.columns
    return (comparison.select(*_select_with_alias("raw", raw_cols), F.col("status"))
                      .withColumn("insert_ts", F.current_timestamp())
                      .withColumn("source_table", F.lit(table_name)))

def detect_deletes(raw_df, raw_prev_df, key_columns, table_name):
    on_cond = build_join_condition("prev", "raw", key_columns)
    deleted_prev = raw_prev_df.alias("prev").join(raw_df.alias("raw"), on=on_cond, how="left_anti")
    raw_cols = raw_df.columns
    return (deleted_prev.select(*_select_with_alias("prev", raw_cols))
                        .withColumn("status", F.lit("D"))
                        .withColumn("insert_ts", F.current_timestamp())
                        .withColumn("source_table", F.lit(table_name)))

# =========================
# WRITES / SWAPS / SNAPSHOTS
# =========================
def write_to_temp_staging(df, table_name):
    temp_staging_table = f"staging.temp_stg_{table_name}"
    by = df.groupBy("status").count().collect() if df.limit(1).count() > 0 else []
    df.write.mode("overwrite").format("delta").saveAsTable(temp_staging_table)
    counts = {"I":0, "U":0, "D":0}
    for r in by:
        counts[r["status"]] = int(r["count"])
    return counts

def finalize_staging_table(table_name):
    temp_staging_table = f"staging.temp_stg_{table_name}"
    final_staging_table = f"staging.stg_{table_name}"
    if table_exists(temp_staging_table) and spark.read.table(temp_staging_table).limit(1).count() > 0:
        spark.sql(f"CREATE OR REPLACE TABLE {final_staging_table} AS SELECT * FROM {temp_staging_table}")
    spark.sql(f"DROP TABLE IF EXISTS {temp_staging_table}")

def move_raw_to_prev_raw(table_name):
    raw_table = f"MHMR_LAKEHOUSE.RAW.{table_name}_RAW"
    prev_raw_table = f"MHMR_LAKEHOUSE.PREV_RAW.{table_name}_RAW_PREV"
    spark.sql(f"CREATE OR REPLACE TABLE {prev_raw_table} AS SELECT * FROM {raw_table}")

def snapshot_prev_raw_table(table_name):
    snapshot_table = f"MHMR_LAKEHOUSE.PREV_RAW_SNAPSHOT.{table_name}_RAW_PREV_{Today_date.strftime('%Y%m%d')}"
    prev_raw_table = f"MHMR_LAKEHOUSE.PREV_RAW.{table_name}_RAW_PREV"
    spark.sql(f"CREATE OR REPLACE TABLE {snapshot_table} AS SELECT * FROM {prev_raw_table}")

# =========================
# PER-TABLE EXECUTION
# =========================
def process_single_table(row):
    table_name = row["TABLE_NAME"]
    key_columns = [k.strip() for k in row["KEY_COLUMN"].split(",")]
    row_hash_col = "row_hash"
    status = get_today_status(table_name)
    if status == "Succeeded":
        return (table_name, "Skipped", {"I":0,"U":0,"D":0}, None)
    begin_daily_log(table_name)
    try:
        raw_df = spark.read.table(f"MHMR_LAKEHOUSE.RAW.{table_name}_RAW").alias("raw")
        raw_prev_df = safe_load_prev_raw_table(table_name)
        if raw_prev_df is None:
            changes_df = (raw_df
                          .select(*_select_with_alias("raw", raw_df.columns))
                          .withColumn("status", F.lit("I"))
                          .withColumn("insert_ts", F.current_timestamp())
                          .withColumn("source_table", F.lit(table_name)))
        else:
            changes_df = detect_inserts_updates(raw_df, raw_prev_df, key_columns, row_hash_col, table_name)
            deletes_df = detect_deletes(raw_df, raw_prev_df, key_columns, table_name)
            if deletes_df.limit(1).count() > 0:
                changes_df = changes_df.unionByName(deletes_df)
        counts = write_to_temp_staging(changes_df, table_name)
        finalize_staging_table(table_name)
        move_raw_to_prev_raw(table_name)
        snapshot_prev_raw_table(table_name)
        complete_daily_log(table_name, "Succeeded", inserts=counts.get("I",0), updates=counts.get("U",0), deletes=counts.get("D",0))
        return (table_name, "Succeeded", counts, None)
    except Exception as e:
        complete_daily_log(table_name, "Failed", 0, 0, 0, error_message=str(e))
        return (table_name, "Failed", {"I":0,"U":0,"D":0}, str(e))

# =========================
# FINAL CONSOLIDATION
# =========================
def consolidate_daily_into_final():
    acquire_lock()
    try:
        spark.sql(f"DELETE FROM {FINAL_LOG} WHERE run_date = DATE('{TODAY_STR}')")
        spark.sql(f"INSERT INTO {FINAL_LOG} SELECT * FROM {DAILY_LOG} WHERE run_date = DATE('{TODAY_STR}')")
        spark.catalog.clearCache()
    finally:
        release_lock()

# =========================
# RUNNER (PARALLEL)
# =========================

def _resolve_max_workers(n_tables, fallback=8):
    """Derive a sensible max_workers from Spark conf / env, capped by table count."""
    # Preferred config path: spark.conf "mhmr.maxWorkers" (settable in Fabric Notebook settings)
    try:
        conf_val = int(spark.conf.get("mhmr.maxWorkers"))
        if conf_val > 0:
            return min(conf_val, max(1, n_tables))
    except Exception:
        pass
    # Fallback to environment variable
    try:
        env_val = int(os.environ.get("MAX_WORKERS", "0"))
        if env_val > 0:
            return min(env_val, max(1, n_tables))
    except Exception:
        pass
    # Sensible default: up to 8 threads, but never more than table count
    return min(fallback, max(1, n_tables))

def process_all_tables_parallel(max_workers=None):
    ensure_log_tables()
    tables_df = load_table_metadata()
    rows = tables_df.collect()
    if max_workers is None:
        max_workers = _resolve_max_workers(len(rows))

    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = [ex.submit(process_single_table, r.asDict()) for r in rows]
        for fut in as_completed(futures):
            results.append(fut.result())

    consolidate_daily_into_final()
    return results

# === RUN ===
results = process_all_tables_parallel()
