from datetime import datetime
import pytz
import time
from pyspark.sql.functions import col
from pyspark.sql import Row
from pyspark.sql.types import StructType, StructField, StringType, DateType, TimestampType
from notebookutils import mssparkutils

# ==== PARAMETERS ====
base_table = table_name           # from pipeline
operation = operation             # from pipeline
status_val = "Succeeded"
error_msg = ""

# ==== TIMEZONE ====
tz = pytz.timezone("America/Chicago")
now = datetime.now(tz)
run_date = now.date()

# ==== PATHS ====
log_path = "Files/BRONZE/STAGING_LOGS/STAGING_JOB_LOGS"
temp_path = "Files/BRONZE/STAGING_LOGS/STAGING_JOB_LOGS_TEMP"
lock_file = "Files/BRONZE/STAGING_LOGS/STAGING_JOB_LOGS_LOCK"
input_path = f"Files/BRONZE/INCREMENTAL_LOAD/{base_table}_{operation}"

# ==== LOG SCHEMA ====
schema = StructType([
    StructField("run_date", DateType(), True),
    StructField("table_name", StringType(), True),
    StructField("operation", StringType(), True),
    StructField("status", StringType(), True),
    StructField("error_message", StringType(), True),
    StructField("UTCTimestamp", TimestampType(), True)
])

# ==== Read processed UTCs from current load ====
try:
    df = spark.read.parquet(input_path)
    raw_utc_dates = df.select("UTCTimestamp").distinct().rdd.flatMap(lambda x: x).collect()
    new_utc_dates = [dt.date() if isinstance(dt, datetime) else None for dt in raw_utc_dates if dt is not None]
except Exception as e:
    print(f"[ERROR] Failed to read UTCs from input: {str(e)}")
    new_utc_dates = []

# ==== Read full existing logs ====
try:
    df_existing = spark.read.schema(schema).parquet(log_path)
except:
    df_existing = spark.createDataFrame([], schema)

table_id = f"{base_table}_{operation}"

# ==== Find already logged UTCs for this table/operation ====
existing_utc_set = df_existing.filter(
    (col("table_name") == table_id) &
    (col("operation") == operation)
).select("UTCTimestamp").distinct().rdd.flatMap(lambda x: x).collect()

existing_utc_set = set([dt.date() if isinstance(dt, datetime) else dt for dt in existing_utc_set if dt is not None])

# ==== Filter only new UTCs ====
utc_dates_to_log = [utc for utc in new_utc_dates if utc not in existing_utc_set]

# ==== Exit if nothing new to log ====
if not utc_dates_to_log:
    print(f"[LOG] All UTCs already logged for {table_id}")
    mssparkutils.notebook.exit("NO-UTC-FOUND")

# ==== Create log rows ====
log_rows = [
    Row(
        run_date=run_date,
        table_name=table_id,
        operation=operation,
        status=status_val,
        error_message=error_msg,
        UTCTimestamp=utc
    ) for utc in utc_dates_to_log
]

# ==== Acquire Lock ====
fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
lock_path_obj = spark._jvm.org.apache.hadoop.fs.Path(lock_file)
start_time = time.time()
timeout = 60

while fs.exists(lock_path_obj):
    if time.time() - start_time > timeout:
        raise TimeoutError("[LOCK] Timeout waiting for STAGING_LOGS lock.")
    time.sleep(5)

fs.create(lock_path_obj)

# ==== Final write: remove matches from full log, union new rows, overwrite ====
try:
    # Remove matching (table, operation, UTC) from full log
    for utc in utc_dates_to_log:
        df_existing = df_existing.filter(~(
            (col("table_name") == table_id) &
            (col("operation") == operation) &
            (col("UTCTimestamp") == utc)
        ))

    # Union full log with new entries
    df_updated = df_existing.unionByName(spark.createDataFrame(log_rows, schema=schema))

    # Write to temp
    df_updated.write.mode("overwrite").parquet(temp_path)

    # Atomic move
    fs_temp = spark._jvm.org.apache.hadoop.fs.Path(temp_path)
    fs_log = spark._jvm.org.apache.hadoop.fs.Path(log_path)

    if fs.exists(fs_temp):
        if fs.exists(fs_log):
            fs.delete(fs_log, True)
        fs.rename(fs_temp, fs_log)
        print(f"[LOG] SUCCESS: {table_id} â†’ {utc_dates_to_log}")
    else:
        raise ValueError("[ERROR] Temp path missing.")
finally:
    fs.delete(lock_path_obj, False)
