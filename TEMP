from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StructField, StringType, DateType, LongType, TimestampType
from delta.tables import DeltaTable
from functools import reduce
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import pytz, os, errno, time

# =========================
# INIT
# =========================
spark = SparkSession.builder.appName("Process History Tables").getOrCreate()
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
spark.conf.set("spark.sql.session.timeZone", "UTC")
spark.conf.set("spark.sql.parquet.int96RebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInWrite", "CORRECTED")

tz = pytz.timezone("America/Chicago")
Today_date = datetime.now(tz).date()
TODAY_STR = Today_date.strftime("%Y-%m-%d")

LOCK_PATH = "/tmp/staging_logs.lock"  # simple file-based lock

DAILY_LOG = "staging.staging_daily_logs"   # one row per (run_date, table_name)
FINAL_LOG = "staging.staging_final_logs"   # consolidated rollup

# =========================
# UTIL / LOCK
# =========================
def acquire_lock(path=LOCK_PATH, retries=60, delay=1.0):
    for _ in range(retries):
        try:
            fd = os.open(path, os.O_CREAT | os.O_EXCL | os.O_WRONLY)
            os.close(fd)
            return True
        except OSError as e:
            if e.errno != errno.EEXIST:
                raise
            time.sleep(delay)
    raise TimeoutError("Could not acquire log lock")

def release_lock(path=LOCK_PATH):
    try:
        os.remove(path)
    except FileNotFoundError:
        pass

def _and_all(conds):
    return reduce(lambda a,b: a & b, conds) if conds else F.lit(True)

def _select_with_alias(src_alias, cols):
    return [F.col(f"{src_alias}.{c}").alias(c) for c in cols]

def table_exists(tbl):
    try:
        spark.read.table(tbl).limit(1).count()
        return True
    except Exception:
        return False

def load_table_metadata():
    return spark.read.table("TABLES_LIST")

# =========================
# LOG TABLE SCHEMA
# =========================
log_schema = StructType([
    StructField("run_date", DateType(), True),
    StructField("table_name", StringType(), True),
    StructField("status", StringType(), True),           # Running | Succeeded | Failed | Skipped
    StructField("inserts", LongType(), True),
    StructField("updates", LongType(), True),
    StructField("deletes", LongType(), True),
    StructField("attempt", LongType(), True),
    StructField("error_message", StringType(), True),
    StructField("start_ts", TimestampType(), True),
    StructField("end_ts", TimestampType(), True),
    StructField("duration_seconds", LongType(), True)    # end_ts - start_ts in seconds
])

# =========================
# CREATE LOG TABLES (NO spark.sql)
# - DAILY_LOG via method #1 (schema-only empty DataFrame)
# - FINAL_LOG via method #2 (emptyRDD + schema)
# =========================
def ensure_log_tables():
    if not spark.catalog.tableExists(DAILY_LOG):
        empty_df_daily = spark.createDataFrame([], log_schema)  # method #1
        empty_df_daily.write.format("delta").mode("overwrite").saveAsTable(DAILY_LOG)

    if not spark.catalog.tableExists(FINAL_LOG):
        rdd = spark.sparkContext.emptyRDD()                      # method #2
        empty_df_final = spark.createDataFrame(rdd, log_schema)
        empty_df_final.write.format("delta").mode("overwrite").saveAsTable(FINAL_LOG)

def _get_delta(tbl_name) -> DeltaTable:
    # Assumes table exists; caller should ensure.
    return DeltaTable.forName(spark, tbl_name)

# =========================
# DAILY LOG UPSERTS (DeltaTable APIs)
# =========================
def begin_daily_log(table_name):
    ensure_log_tables()
    acquire_lock()
    try:
        now_ts = datetime.now(tz)
        src = spark.createDataFrame(
            [(Today_date, table_name, now_ts)],
            schema="run_date date, table_name string, now_ts timestamp"
        )

        dt = _get_delta(DAILY_LOG)
        (
            dt.alias("t")
            .merge(
                src.alias("s"),
                "t.run_date = s.run_date AND t.table_name = s.table_name"
            )
            .whenMatchedUpdate(set={
                "status": F.lit("Running"),
                "attempt": F.coalesce(F.col("t.attempt"), F.lit(0)) + F.lit(1),
                "error_message": F.lit(None).cast("string"),
                "start_ts": F.coalesce(F.col("t.start_ts"), F.col("s.now_ts")),
                "end_ts": F.lit(None).cast("timestamp"),
                "duration_seconds": F.lit(None).cast("long"),
            })
            .whenNotMatchedInsert(values={
                "run_date": F.col("s.run_date"),
                "table_name": F.col("s.table_name"),
                "status": F.lit("Running"),
                "inserts": F.lit(None).cast("long"),
                "updates": F.lit(None).cast("long"),
                "deletes": F.lit(None).cast("long"),
                "attempt": F.lit(1),
                "error_message": F.lit(None).cast("string"),
                "start_ts": F.col("s.now_ts"),
                "end_ts": F.lit(None).cast("timestamp"),
                "duration_seconds": F.lit(None).cast("long"),
            })
            .execute()
        )
    finally:
        release_lock()

def complete_daily_log(table_name, status, inserts=0, updates=0, deletes=0, error_message=None):
    ensure_log_tables()
    acquire_lock()
    try:
        dt = _get_delta(DAILY_LOG)
        # compute end_ts and duration from start_ts
        dt.update(
            condition=(F.col("run_date") == F.lit(Today_date)) & (F.col("table_name") == F.lit(table_name)),
            set={
                "status": F.lit(status),
                "inserts": F.lit(int(inserts)),
                "updates": F.lit(int(updates)),
                "deletes": F.lit(int(deletes)),
                "error_message": F.lit(error_message).cast("string"),
                "end_ts": F.current_timestamp(),
                "duration_seconds": (
                    (F.unix_timestamp(F.current_timestamp()) - F.unix_timestamp(F.col("start_ts"))).cast("long")
                ),
            }
        )
    finally:
        release_lock()

def get_today_status(table_name):
    if not table_exists(DAILY_LOG):
        return None
    df = (
        spark.read.table(DAILY_LOG)
        .where((F.col("run_date") == F.lit(Today_date)) & (F.col("table_name") == F.lit(table_name)))
        .select("status")
        .limit(1)
    )
    rows = df.collect()
    return rows[0]["status"] if rows else None

# =========================
# CHANGE DETECTION
# =========================
def safe_load_prev_raw_table(table_name):
    try:
        df = spark.read.table(f"MHMR_LAKEHOUSE.PREV_RAW.{table_name}_RAW_PREV")
        if df.limit(1).count() == 0:
            return None
        return df.alias("prev")
    except Exception:
        return None

def build_join_condition(df1_alias, df2_alias, key_columns):
    keys = [k.strip() for k in key_columns]
    return _and_all([F.col(f"{df1_alias}.{k}") == F.col(f"{df2_alias}.{k}") for k in keys])

def detect_inserts_updates(raw_df, raw_prev_df, key_columns, row_hash_col, table_name):
    on_cond = build_join_condition("raw", "prev", key_columns)
    joined = raw_df.alias("raw").join(raw_prev_df.alias("prev"), on=on_cond, how="left")
    comparison = (
        joined.withColumn(
            "status",
            F.when(F.col(f"prev.{row_hash_col}").isNull(), F.lit("I"))
             .when(F.col(f"raw.{row_hash_col}") != F.col(f"prev.{row_hash_col}"), F.lit("U"))
        ).filter(F.col("status").isNotNull())
    )
    raw_cols = raw_df.columns
    return (comparison.select(*_select_with_alias("raw", raw_cols), F.col("status"))
                      .withColumn("insert_ts", F.current_timestamp())
                      .withColumn("source_table", F.lit(table_name)))

def detect_deletes(raw_df, raw_prev_df, key_columns, table_name):
    on_cond = build_join_condition("prev", "raw", key_columns)
    deleted_prev = raw_prev_df.alias("prev").join(raw_df.alias("raw"), on=on_cond, how="left_anti")
    raw_cols = raw_df.columns
    return (deleted_prev.select(*_select_with_alias("prev", raw_cols))
                        .withColumn("status", F.lit("D"))
                        .withColumn("insert_ts", F.current_timestamp())
                        .withColumn("source_table", F.lit(table_name)))

# =========================
# WRITES / SWAPS / SNAPSHOTS
# (CTAS kept for simplicity; can be converted to DataFrame writes if you prefer)
# =========================
def write_to_temp_staging(df, table_name):
    temp_staging_table = f"staging.temp_stg_{table_name}"
    by = df.groupBy("status").count().collect() if df.limit(1).count() > 0 else []
    df.write.mode("overwrite").format("delta").saveAsTable(temp_staging_table)
    counts = {"I":0, "U":0, "D":0}
    for r in by:
        counts[r["status"]] = int(r["count"])
    return counts

def finalize_staging_table(table_name):
    temp_staging_table = f"staging.temp_stg_{table_name}"
    final_staging_table = f"staging.stg_{table_name}"
    if table_exists(temp_staging_table) and spark.read.table(temp_staging_table).limit(1).count() > 0:
        spark.sql(f"CREATE OR REPLACE TABLE {final_staging_table} AS SELECT * FROM {temp_staging_table}")
    spark.sql(f"DROP TABLE IF EXISTS {temp_staging_table}")

def move_raw_to_prev_raw(table_name):
    raw_table = f"MHMR_LAKEHOUSE.RAW.{table_name}_RAW"
    prev_raw_table = f"MHMR_LAKEHOUSE.PREV_RAW.{table_name}_RAW_PREV"
    spark.sql(f"CREATE OR REPLACE TABLE {prev_raw_table} AS SELECT * FROM {raw_table}")

def snapshot_prev_raw_table(table_name):
    snapshot_table = f"MHMR_LAKEHOUSE.PREV_RAW_SNAPSHOT.{table_name}_RAW_PREV_{Today_date.strftime('%Y%m%d')}"
    prev_raw_table = f"MHMR_LAKEHOUSE.PREV_RAW.{table_name}_RAW_PREV"
    spark.sql(f"CREATE OR REPLACE TABLE {snapshot_table} AS SELECT * FROM {prev_raw_table}")

# =========================
# PER-TABLE EXECUTION WITH RETRY/IDEMPOTENCY
# =========================
def process_single_table(row):
    table_name = row["TABLE_NAME"]
    key_columns = [k.strip() for k in row["KEY_COLUMN"].split(",")]
    row_hash_col = "row_hash"

    # Skip if already succeeded today
    status = get_today_status(table_name)
    if status == "Succeeded":
        return (table_name, "Skipped", {"I":0,"U":0,"D":0}, None)

    begin_daily_log(table_name)

    try:
        raw_df = spark.read.table(f"MHMR_LAKEHOUSE.RAW.{table_name}_RAW").alias("raw")
        raw_prev_df = safe_load_prev_raw_table(table_name)

        if raw_prev_df is None:
            changes_df = (raw_df
                          .select(*_select_with_alias("raw", raw_df.columns))
                          .withColumn("status", F.lit("I"))
                          .withColumn("insert_ts", F.current_timestamp())
                          .withColumn("source_table", F.lit(table_name)))
        else:
            changes_df = detect_inserts_updates(raw_df, raw_prev_df, key_columns, row_hash_col, table_name)
            deletes_df = detect_deletes(raw_df, raw_prev_df, key_columns, table_name)
            if deletes_df.limit(1).count() > 0:
                changes_df = changes_df.unionByName(deletes_df)

        counts = write_to_temp_staging(changes_df, table_name)
        finalize_staging_table(table_name)
        move_raw_to_prev_raw(table_name)
        snapshot_prev_raw_table(table_name)

        complete_daily_log(
            table_name,
            "Succeeded",
            inserts=counts.get("I",0),
            updates=counts.get("U",0),
            deletes=counts.get("D",0)
        )
        return (table_name, "Succeeded", counts, None)

    except Exception as e:
        complete_daily_log(table_name, "Failed", 0, 0, 0, error_message=str(e))
        return (table_name, "Failed", {"I":0,"U":0,"D":0}, str(e))

# =========================
# FINAL CONSOLIDATION (DeltaTable APIs)
# =========================
def consolidate_daily_into_final():
    ensure_log_tables()
    acquire_lock()
    try:
        # Delete today's slice from FINAL
        dt_final = _get_delta(FINAL_LOG)
        dt_final.delete(condition=(F.col("run_date") == F.lit(Today_date)))

        # Insert today's rows from DAILY
        todays = spark.read.table(DAILY_LOG).where(F.col("run_date") == F.lit(Today_date))
        todays.write.format("delta").mode("append").saveAsTable(FINAL_LOG)

        spark.catalog.clearCache()  # keep Fabric reads fresh
    finally:
        release_lock()

# =========================
# RUNNER (PARALLEL)
# =========================
def _resolve_max_workers(n_tables, fallback=8):
    # Prefer spark.conf "mhmr.maxWorkers" or env MAX_WORKERS; cap by table count
    try:
        conf_val = int(spark.conf.get("mhmr.maxWorkers"))
        if conf_val > 0:
            return min(conf_val, max(1, n_tables))
    except Exception:
        pass
    try:
        env_val = int(os.environ.get("MAX_WORKERS", "0"))
        if env_val > 0:
            return min(env_val, max(1, n_tables))
    except Exception:
        pass
    return min(fallback, max(1, n_tables))

def process_all_tables_parallel(max_workers=None):
    ensure_log_tables()
    tables_df = load_table_metadata()
    rows = tables_df.collect()
    if max_workers is None:
        max_workers = _resolve_max_workers(len(rows))

    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = [ex.submit(process_single_table, r.asDict()) for r in rows]
        for fut in as_completed(futures):
            results.append(fut.result())

    consolidate_daily_into_final()
    return results

# === RUN ===
results = process_all_tables_parallel()
# Optional interactive summary:
# for t, status, counts, err in results:
#     print(f"{t}: {status} (I={counts.get('I',0)}, U={counts.get('U',0)}, D={counts.get('D',0)}){'' if not err else ' | ' + err}")
