from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sha2, concat_ws, when, lit, date_format, to_date
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timezone
import pytz
import threading
import traceback
import os
import uuid
from notebookutils import mssparkutils

# === INIT SPARK SESSION ===
spark = SparkSession.builder.appName("Process Incremental Tables").getOrCreate()

# === TIMEZONE SETTINGS ===
spark.conf.set("spark.sql.session.timeZone", "UTC")
spark.conf.set("spark.sql.parquet.int96RebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInWrite", "CORRECTED")

# === SET CST Load Date ===
tz = pytz.timezone("America/Chicago")
Today_date = datetime.now(tz).strftime("%Y-%m-%d")  # Used for load_date field in logs

# === CONFIG PATHS ===
staging_path = "Files/BRONZE/INCREMENTAL_LOAD"
copy_path = "Files/BRONZE/HISTORY"
table_list_path = "Files/TABLES_LIST"
daily_log_path = "Files/BRONZE/HISTORY/LOGS/Daily_History_Logs"
main_log_path = "Files/BRONZE/HISTORY/LOGS/Final_History_Logs"
lock_file = "/tmp/update_log.lock"

# === FILE LOCK ===
def acquire_lock():
    if os.path.exists(lock_file):
        return False
    with open(lock_file, 'w') as f:
        f.write("locked")
    return True

def release_lock():
    if os.path.exists(lock_file):
        os.remove(lock_file)

# === SAFE READ WITH DEBUG ===
def safe_read_parquet(path):
    print(f"[DEBUG] Checking path: {path}")
    if not mssparkutils.fs.exists(path):
        print(f"[SKIP] File not found: {path}")
        return None
    try:
        df = spark.read.parquet(path)
        print(f"[INFO] Successfully read: {path} | Rows: {df.count()} | Columns: {df.columns}")
        return df
    except Exception as e:
        print(f"[ERROR] Failed to read {path}: {str(e)}")
        return None

# === HASHING ===
def add_hash_column(df):
    cleaned_cols = []
    for field in df.schema.fields:
        col_name = field.name
        cleaned_col = when(col(col_name).isNull(), lit(" ")).otherwise(col(col_name).cast("string"))
        cleaned_cols.append(cleaned_col.alias(col_name))
    temp_df = df.select(*cleaned_cols)
    row_hash_col = sha2(concat_ws("||", *temp_df.columns), 256).alias("ROW_HASH")
    return df.withColumn("ROW_HASH", row_hash_col)

# === LOG WRITER ===
def update_log(table_name, status, utc_timestamp_str, start_time, end_time, count=0, error_message=""):
    print(f"[LOG] Updating log | Table: {table_name} | Status: {status} | UTC: {utc_timestamp_str} | Count: {count}")
    try:
        utc_date = datetime.strptime(utc_timestamp_str, "%Y-%m-%d").date()
        utc_ts_full = datetime.combine(utc_date, datetime.min.time()).replace(tzinfo=timezone.utc)
        run_date = datetime.now(pytz.timezone("America/Chicago")).date()
        new_row = [(table_name, status, run_date, utc_ts_full, start_time.isoformat(), end_time.isoformat(), count, error_message)]

        try:
            schema = spark.read.parquet(daily_log_path).schema
        except:
            schema = spark.read.parquet(main_log_path).schema
            spark.createDataFrame([], schema).write.mode("overwrite").parquet(daily_log_path)

        new_df = spark.createDataFrame(new_row, schema)

        while not acquire_lock():
            continue
        try:
            try:
                existing_df = spark.read.parquet(daily_log_path)
                filtered_df = existing_df.filter(~(
                    (col("table_name") == table_name) &
                    (col("load_date") == run_date) &
                    (to_date(col("UTCTimestamp")) == utc_date) &
                    (col("status") == status)
                ))
                final_df = filtered_df.unionByName(new_df)
            except:
                final_df = new_df

            temp_daily_path = f"{daily_log_path}_temp_{uuid.uuid4()}"
            final_df.write.mode("overwrite").parquet(temp_daily_path)

            fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
            Path = spark._jvm.org.apache.hadoop.fs.Path
            if fs.exists(Path(daily_log_path)):
                fs.delete(Path(daily_log_path), True)
            fs.rename(Path(temp_daily_path), Path(daily_log_path))
        finally:
            release_lock()
    except Exception as e:
        print(f"[LOG ERROR] Failed to update log for {table_name} | Status: {status}")
        print(traceback.format_exc())

# === EXTRACT UTCs ===
def get_unprocessed_utcs_for_table(table_name, log_df):
    insert_path = f"{staging_path}/{table_name}_INSERT"
    print(f"[DEBUG] Reading insert path for {table_name}: {insert_path}")
    insert_df = safe_read_parquet(insert_path)
    if insert_df is None:
        print(f"[WARN] No insert data found for {table_name}")
        return []
    print(f"[TRACE] Extracting UTCs from insert data for {table_name}")
    utc_dates_in_data = insert_df.select(date_format(col("UTCTimestamp"), "yyyy-MM-dd")).distinct().rdd.flatMap(lambda x: x).collect()
    print(f"[DEBUG] UTC dates from insert data: {utc_dates_in_data}")

    logged_utcs = log_df.filter(col("table_name") == table_name)\
                        .select("UTCTimestamp")\
                        .rdd.flatMap(lambda x: x)\
                        .map(lambda d: d.strftime("%Y-%m-%d")).distinct().collect()
    print(f"[DEBUG] UTC dates already logged: {logged_utcs}")

    unprocessed = [utc for utc in utc_dates_in_data if utc not in logged_utcs]
    print(f"[INFO] Unprocessed UTCs for {table_name}: {unprocessed}")
    return unprocessed
# === TABLE LOAD FUNCTION ===
def load_table(table_name, key_column):
    thread_name = threading.current_thread().name
    start_time_total = datetime.now()
    print(f"\n[START] Table: {table_name} | Thread: {thread_name}")

    try:
        copy_table_path = f"{copy_path}/{table_name}_HIST"
        temp_table_path = f"{copy_path}/{table_name}_HIST_TEMP"
        delete_storage_path = f"{copy_path}/{table_name}_HIST_DELETE"

        print(f"[INFO] Reading INSERT: {staging_path}/{table_name}_INSERT")
        insert_df = spark.read.parquet(f"{staging_path}/{table_name}_INSERT")

        print(f"[INFO] Reading UPDATE: {staging_path}/{table_name}_UPDATE")
        update_df = spark.read.parquet(f"{staging_path}/{table_name}_UPDATE")

        print(f"[INFO] Reading DELETE: {staging_path}/{table_name}_DELETE")
        delete_df = spark.read.parquet(f"{staging_path}/{table_name}_DELETE")

        utc_days = insert_df.select(date_format(col("INSERT_TIMESTAMP"), "yyyy-MM-dd").alias("utc_day")) \
                            .distinct().rdd.flatMap(lambda x: x).collect()
        print(f"[INFO] UTC partitions found: {utc_days}")

        if not utc_days:
            print(f"[SKIP] No UTCs found for {table_name}")
            return

        for utc_day in utc_days:
            print(f"\n[PROCESSING UTC] {utc_day} for table {table_name}")
            try:
                copy_table = safe_read_parquet(copy_table_path)
                if copy_table is None:
                    copy_table = spark.createDataFrame([], insert_df.schema.add("flag", "string"))
                    print("[INFO] Initialized empty historical table")

                delete_view = delete_df.filter(date_format(col("INSERT_TIMESTAMP"), "yyyy-MM-dd") == utc_day)
                if not delete_view.rdd.isEmpty():
                    print(f"[INFO] Deleting records for UTC {utc_day}")
                    delete_keys = delete_view.select(key_column).distinct()
                    deleted_records = copy_table.join(delete_keys, key_column, "inner") \
                        .withColumn("flag", lit("I")) \
                        .withColumn("INSERT_TIMESTAMP", lit(datetime.now(timezone.utc)))
                    deleted_records = add_hash_column(deleted_records)
                    deleted_records.write.mode("append").parquet(delete_storage_path)
                else:
                    print(f"[INFO] No deletes for {table_name} on UTC {utc_day}")
                    deleted_records = spark.createDataFrame([], copy_table.schema)

                insert_view = insert_df.filter(date_format(col("INSERT_TIMESTAMP"), "yyyy-MM-dd") == utc_day) \
                    .withColumn("flag", lit("A"))
                update_view = update_df.filter(date_format(col("INSERT_TIMESTAMP"), "yyyy-MM-dd") == utc_day) \
                    .withColumn("flag", lit("A"))

                merged_view = deleted_records.unionByName(insert_view, allowMissingColumns=True) \
                                             .unionByName(update_view, allowMissingColumns=True)

                if not merged_view.rdd.isEmpty():
                    print(f"[INFO] Merging records into {table_name}_HIST")
                    copy_table = copy_table.unionByName(merged_view, allowMissingColumns=True)
                    copy_table = add_hash_column(copy_table)

                fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
                Path = spark._jvm.org.apache.hadoop.fs.Path
                temp_path = Path(temp_table_path)
                final_path = Path(copy_table_path)
                if fs.exists(temp_path): fs.delete(temp_path, True)
                copy_table.write.mode("overwrite").parquet(temp_table_path)
                if fs.exists(final_path): fs.delete(final_path, True)
                fs.rename(temp_path, final_path)

                end_time = datetime.now()
                update_log(table_name, "D", utc_day, start_time_total, end_time, deleted_records.count())
                update_log(table_name, "I", utc_day, start_time_total, end_time, insert_view.count())
                update_log(table_name, "U", utc_day, start_time_total, end_time, update_view.count())
            except Exception as e:
                update_log(table_name, "FAILURE", utc_day, start_time_total, datetime.now(), 0, traceback.format_exc())
    except Exception as e:
        update_log(table_name, "FAILURE", Today_date, start_time_total, datetime.now(), 0, traceback.format_exc())

# === LOAD TABLE LIST ===
print("[INFO] Reading active table list")
table_list_df = spark.read.parquet(table_list_path)
active_tables_df = table_list_df.filter("STATUS = 'A'")
active_tables = [(row["TABLE_NAME"], row["KEY_COLUMN"]) for row in active_tables_df.collect()]

# === BUILD TO-RUN LIST ===
tables_to_run = []
try:
    print("[INFO] Reading main log for retry check")
    main_log_df = spark.read.parquet(main_log_path)
    failed_today_df = main_log_df.filter((col("load_date") == Today_date) & (col("status") == "FAILURE"))
    failed_today_tables = [row["table_name"] for row in failed_today_df.collect()]

    if failed_today_tables:
        print("[MODE] Retrying failed tables")
        for table_name, key_column in active_tables:
            if table_name in failed_today_tables:
                utc_days = get_unprocessed_utcs_for_table(table_name, main_log_df)
                if utc_days:
                    tables_to_run.append((table_name, key_column))
    else:
        print("[MODE] Checking all active tables")
        for table_name, key_column in active_tables:
            utc_days = get_unprocessed_utcs_for_table(table_name, main_log_df)
            if utc_days:
                tables_to_run.append((table_name, key_column))

    if not tables_to_run:
        print("[INFO] All tables already processed. Exiting.")
except Exception as e:
    print(f"[ERROR] Log read failed, defaulting to all tables. Reason: {e}")
    for table_name, key_column in active_tables:
        utc_days = get_unprocessed_utcs_for_table(table_name, spark.createDataFrame([], schema=None))
        if utc_days:
            tables_to_run.append((table_name, key_column))

# === THREAD EXECUTION ===
if tables_to_run:
    print(f"[INFO] Tables to process: {tables_to_run}")
    with ThreadPoolExecutor(max_workers=8) as executor:
        futures = {
            executor.submit(load_table, table, key): table
            for table, key in tables_to_run
        }
        for future in as_completed(futures):
            print("[THREAD COMPLETE]")
else:
    print("[INFO] Nothing to process.")

# === FINAL LOG MERGE ===
print("[MERGE] Starting final log merge")


# === READ LOGS ===
try:
    main_log_df = spark.read.parquet(main_log_path)
except:
    print("[INFO] main_log_path not found. Initializing empty log.")
    main_log_df = spark.createDataFrame([], schema=None)

try:
    daily_log_df = spark.read.parquet(daily_log_path)
except:
    print("[INFO] daily_log_path not found. Initializing empty log.")
    daily_log_df = spark.createDataFrame([], main_log_df.schema)

# === IF daily_log_df IS EMPTY, WRITE main_log_df AS-IS
if daily_log_df.rdd.isEmpty():
    final_main_log_df = main_log_df
else:
    # === STEP 1: Remove old FAILURE if same UTC+table succeeded now (I/U/D)
    success_df = daily_log_df.filter(col("status").isin(["I", "U", "D"]))
    if not success_df.rdd.isEmpty():
        failure_keys = success_df.select("table_name", "UTCTimestamp").distinct()
        main_log_df = main_log_df.join(
            failure_keys,
            on=["table_name", "UTCTimestamp"],
            how="left_anti"
        ).unionByName(
            main_log_df.join(
                failure_keys,
                on=["table_name", "UTCTimestamp"],
                how="inner"
            ).filter(~col("status").isin(["FAILURE"]))
        )

    # === STEP 2: Remove exact duplicates (same table_name, UTCTimestamp, status)
    join_keys = ["table_name", "UTCTimestamp", "status"]
    main_filtered_df = main_log_df.alias("main").join(
        daily_log_df.select(*join_keys).distinct().alias("daily"),
        on=join_keys,
        how="left_anti"
    )

    # === STEP 3: Merge daily log into cleaned main log
    final_main_log_df = main_filtered_df.unionByName(daily_log_df)

# === WRITE FINAL LOG WITH ATOMIC RENAME
temp_main_log_path = main_log_path + "_tmp"
final_main_log_df.write.mode("overwrite").parquet(temp_main_log_path)

fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
Path = spark._jvm.org.apache.hadoop.fs.Path

if fs.exists(Path(main_log_path)):
    fs.delete(Path(main_log_path), True)
fs.rename(Path(temp_main_log_path), Path(main_log_path))

# === CLEAN UP DAILY LOG
if fs.exists(Path(daily_log_path)):
    fs.delete(Path(daily_log_path), True)

print("[COMPLETE] Final history log safely merged with retry replacement logic.")
