# FINAL FABRIC SCRIPT: Process History Tables with Logs, Hash Compare, Retry, ThreadPool, and Prints

from pyspark.sql import SparkSession, functions as F, types as T
from datetime import datetime
import pytz, traceback
from concurrent.futures import ThreadPoolExecutor, as_completed

# === INIT SPARK SESSION ===
spark = SparkSession.builder.appName("Process History Tables").getOrCreate()
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
spark.conf.set("spark.sql.session.timeZone", "UTC")
spark.conf.set("spark.sql.parquet.int96RebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInWrite", "CORRECTED")

# === CONFIG ===
tz = pytz.timezone("America/Chicago")
Today_date = datetime.now(tz).date()

def _ts():
    return datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")

# === LOG TABLE CREATION ===
def ensure_log_tables_exist():
    print(f"[{_ts()}] Checking log tables existence...")
    try:
        spark.read.table("logs.daily_log")
        print(f"[{_ts()}] logs.daily_log exists.")
    except:
        print(f"[{_ts()}] logs.daily_log not found. Creating empty table...")
        schema = T.StructType([
            T.StructField("run_date", T.DateType()),
            T.StructField("table_name", T.StringType()),
            T.StructField("status", T.StringType()),
            T.StructField("error_message", T.StringType()),
            T.StructField("insert_timestamp", T.TimestampType()),
            T.StructField("start_time", T.TimestampType()),
            T.StructField("end_time", T.TimestampType()),
            T.StructField("duration_sec", T.DoubleType())
        ])
        empty_df = spark.createDataFrame([], schema)
        empty_df.write.mode("overwrite").saveAsTable("logs.daily_log")
        print(f"[{_ts()}] logs.daily_log created.")

# === LOGGING FUNCTIONS ===
def begin_daily_log(table_name):
    print("\n" + "="*60)
    print(f"[{_ts()}] STARTING PROCESS FOR TABLE: {table_name}")
    start_time = datetime.now(tz)
    log_df = spark.createDataFrame([
        (Today_date, table_name, "Running", None, start_time, start_time, None)
    ], ["run_date","table_name","status","error_message","insert_timestamp","start_time","end_time","duration_sec"])
    log_df.write.mode("append").saveAsTable("logs.daily_log")
    return start_time

def complete_daily_log(table_name, status, start_time, error_message=None):
    end_time = datetime.now(tz)
    duration = (end_time - start_time).total_seconds()
    print(f"[{_ts()}] COMPLETED TABLE: {table_name} | STATUS: {status} | Duration: {duration:.2f} seconds")
    print("="*60 + "\n")
    upd_df = spark.createDataFrame([
        (Today_date, table_name, status, error_message, end_time, start_time, end_time, duration)
    ], ["run_date","table_name","status","error_message","insert_timestamp","start_time","end_time","duration_sec"])
    upd_df.write.mode("append").saveAsTable("logs.daily_log")

# === CHANGE DETECTION ===
def build_changes(raw_df, prev_df, key_columns, row_hash_col, table_name):
    print(f"[{_ts()}] Building change set for: {table_name}")
    join_keys = [F.col(f"raw.{k}") == F.col(f"prev.{k}") for k in key_columns]

    print(f"[{_ts()}] Detecting deletes...")
    deletes = (
        prev_df.alias("prev")
        .join(raw_df.alias("raw"), on=join_keys, how="left_anti")
        .select("prev.*")
        .withColumn("status", F.lit("D"))
        .withColumn("insert_ts", F.current_timestamp())
        .withColumn("source_table", F.lit(table_name))
    )

    print(f"[{_ts()}] Detecting updates...")
    updates = (
        raw_df.alias("raw")
        .join(prev_df.alias("prev"), on=join_keys, how="inner")
        .filter(F.col(f"raw.{row_hash_col}") != F.col(f"prev.{row_hash_col}"))
        .select("raw.*")
        .withColumn("status", F.lit("U"))
        .withColumn("insert_ts", F.current_timestamp())
        .withColumn("source_table", F.lit(table_name))
    )

    print(f"[{_ts()}] Detecting inserts...")
    inserts = (
        raw_df.alias("raw")
        .join(prev_df.alias("prev"), on=join_keys, how="left")
        .filter(F.col(f"prev.{key_columns[0]}").isNull())
        .select("raw.*")
        .withColumn("status", F.lit("I"))
        .withColumn("insert_ts", F.current_timestamp())
        .withColumn("source_table", F.lit(table_name))
    )

    if deletes.rdd.isEmpty() and updates.rdd.isEmpty() and inserts.rdd.isEmpty():
        print(f"[{_ts()}] No changes found for table {table_name}.")
        return None

    print(f"[{_ts()}] Change detection complete: D={deletes.count()}, U={updates.count()}, I={inserts.count()}")
    return deletes.unionByName(updates).unionByName(inserts)

# === TABLE WRITES ===
def write_to_temp_staging(df, table_name):
    temp_table = f"staging.temp_stg_{table_name}"
    print(f"[{_ts()}] Writing data to: {temp_table}")
    df.write.mode("overwrite").saveAsTable(temp_table)
    print(f"[{_ts()}] Data written to temp staging table.")

def promote_to_final_staging(table_name):
    temp_table = f"staging.temp_stg_{table_name}"
    final_table = f"staging.stg_{table_name}"
    print(f"[{_ts()}] Promoting {temp_table} to {final_table}")
    spark.sql(f"DROP TABLE IF EXISTS {final_table}")
    spark.sql(f"CREATE TABLE {final_table} AS SELECT * FROM {temp_table}")
    spark.sql(f"DROP TABLE IF EXISTS {temp_table}")
    print(f"[{_ts()}] Promotion to final staging completed.")

# === TABLE PROCESSING ===
def process_single_table(row):
    table_name = row["TABLE_NAME"]
    key_columns = row["KEY_COLUMN"].split(',')
    row_hash_col = "row_hash"
    start_time = begin_daily_log(table_name)
    try:
        print(f"[{_ts()}] Reading RAW and PREV_RAW for {table_name}...")
        raw_df = spark.read.table(f"MHMR_LAKEHOUSE.RAW.{table_name}_RAW")
        prev_df = spark.read.table(f"MHMR_LAKEHOUSE.PREV_RAW.{table_name}_RAW_PREV")

        changes_df = build_changes(raw_df, prev_df, key_columns, row_hash_col, table_name)

        if changes_df is None:
            complete_daily_log(table_name, "Succeeded", start_time)
            return

        write_to_temp_staging(changes_df, table_name)
        promote_to_final_staging(table_name)
        complete_daily_log(table_name, "Succeeded", start_time)

    except Exception as e:
        print(f"[{_ts()}] Failed to process {table_name}: {str(e)}")
        traceback.print_exc()
        complete_daily_log(table_name, "Failed", start_time, error_message=str(e))

# === PROCESS ALL TABLES ===
def process_all_tables():
    ensure_log_tables_exist()
    print(f"[{_ts()}] Reading TABLES_LIST...")
    tables_list = spark.read.table("TABLES_LIST").collect()
    print(f"[{_ts()}] Found {len(tables_list)} tables to process.")
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(process_single_table, row.asDict()) for row in tables_list]
        for future in as_completed(futures):
            future.result()

# === RUN ===
process_all_tables()
