
from pyspark.sql import SparkSession, functions as F, types as T
from datetime import datetime
import pytz, traceback
from concurrent.futures import ThreadPoolExecutor, as_completed

# === INIT SPARK SESSION ===
spark = SparkSession.builder.appName("Process History Tables").getOrCreate()
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)
spark.conf.set("spark.sql.session.timeZone", "UTC")
spark.conf.set("spark.sql.parquet.int96RebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.parquet.int96RebaseModeInWrite", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInRead", "CORRECTED")
spark.conf.set("spark.sql.legacy.parquet.datetimeRebaseModeInWrite", "CORRECTED")

# === CONFIG ===
tz = pytz.timezone("America/Chicago")
Today_date = datetime.now(tz).date()

def _ts():
    return datetime.now(tz).strftime("%Y-%m-%d %H:%M:%S")

# === LOG TABLE CREATION ===
def ensure_log_tables_exist():
    print(f"[{_ts()}] Checking log tables existence...")
    schema = T.StructType([
        T.StructField("run_date", T.DateType()),
        T.StructField("table_name", T.StringType()),
        T.StructField("status", T.StringType()),
        T.StructField("error_message", T.StringType()),
        T.StructField("insert_timestamp", T.TimestampType()),
        T.StructField("start_time", T.TimestampType()),
        T.StructField("end_time", T.TimestampType()),
        T.StructField("duration_sec", T.DoubleType())
    ])
    try:
        spark.read.table("logs.daily_log")
        print(f"[{_ts()}] logs.daily_log exists.")
    except:
        print(f"[{_ts()}] logs.daily_log not found. Creating...")
        spark.createDataFrame([], schema).write.mode("overwrite").saveAsTable("logs.daily_log")

    try:
        spark.read.table("logs.final_log")
        print(f"[{_ts()}] logs.final_log exists.")
    except:
        print(f"[{_ts()}] logs.final_log not found. Creating...")
        spark.createDataFrame([], schema).write.mode("overwrite").saveAsTable("logs.final_log")

# === LOGGING ===
def begin_daily_log(table_name):
    print("\n" + "="*60)
    print(f"[{_ts()}] START TABLE: {table_name}")
    now = datetime.now(tz)
    df = spark.createDataFrame([
        (Today_date, table_name, "Running", None, now, now, now, None)
    ], ["run_date", "table_name", "status", "error_message", "insert_timestamp", "start_time", "end_time", "duration_sec"])
    df.write.mode("append").saveAsTable("logs.daily_log")
    return now

def complete_daily_log(table_name, status, start_time, error_message=None):
    end_time = datetime.now(tz)
    dur = (end_time - start_time).total_seconds()
    print(f"[{_ts()}] END TABLE: {table_name} | STATUS: {status} | Duration: {dur:.2f}s")
    print("="*60 + "\n")
    df = spark.createDataFrame([
        (Today_date, table_name, status, error_message, end_time, start_time, end_time, dur)
    ], ["run_date", "table_name", "status", "error_message", "insert_timestamp", "start_time", "end_time", "duration_sec"])
    df.write.mode("append").saveAsTable("logs.daily_log")

# === DELTA LOGIC ===
def build_changes(raw_df, prev_df, key_columns, row_hash_col, table_name):
    print(f"[{_ts()}] Detecting changes for: {table_name}")
    keys = [F.col(f"raw.{k}") == F.col(f"prev.{k}") for k in key_columns]

    deletes = prev_df.alias("prev").join(raw_df.alias("raw"), keys, "left_anti")\
        .select("prev.*").withColumn("status", F.lit("D")).withColumn("insert_ts", F.current_timestamp())\
        .withColumn("source_table", F.lit(table_name))

    updates = raw_df.alias("raw").join(prev_df.alias("prev"), keys, "inner")\
        .filter(F.col(f"raw.{row_hash_col}") != F.col(f"prev.{row_hash_col}"))\
        .select("raw.*").withColumn("status", F.lit("U")).withColumn("insert_ts", F.current_timestamp())\
        .withColumn("source_table", F.lit(table_name))

    inserts = raw_df.alias("raw").join(prev_df.alias("prev"), keys, "left")\
        .filter(F.col(f"prev.{key_columns[0]}").isNull())\
        .select("raw.*").withColumn("status", F.lit("I")).withColumn("insert_ts", F.current_timestamp())\
        .withColumn("source_table", F.lit(table_name))

    if deletes.rdd.isEmpty() and updates.rdd.isEmpty() and inserts.rdd.isEmpty():
        print(f"[{_ts()}] No changes found.")
        return None

    print(f"[{_ts()}] Changes detected.")
    return deletes.unionByName(updates).unionByName(inserts)

def write_to_temp(df, table_name):
    print(f"[{_ts()}] Writing to staging.temp_stg_{table_name}")
    df.write.mode("overwrite").saveAsTable(f"staging.temp_stg_{table_name}")

def promote_to_final(table_name):
    print(f"[{_ts()}] Promoting to staging.stg_{table_name}")
    spark.sql(f"DROP TABLE IF EXISTS staging.stg_{table_name}")
    spark.sql(f"CREATE TABLE staging.stg_{table_name} AS SELECT * FROM staging.temp_stg_{table_name}")
    spark.sql(f"DROP TABLE IF EXISTS staging.temp_stg_{table_name}")

# === FINAL LOG REPLACEMENT ===
def consolidate_logs_to_final():
    print(f"[{_ts()}] Consolidating logs.daily_log into logs.final_log (atomic)...")
    df = spark.read.table("logs.daily_log").filter(F.col("run_date") == Today_date)
    df.cache()
    df.count()
    temp_table = "logs._final_log_temp"
    df.write.mode("overwrite").saveAsTable(temp_table)
    spark.sql("DROP TABLE IF EXISTS logs.final_log")
    spark.sql("ALTER TABLE logs._final_log_temp RENAME TO final_log")
    print(f"[{_ts()}] Final log updated.")

# === PROCESS ===
def process_table(row):
    table_name = row["TABLE_NAME"]
    keys = row["KEY_COLUMN"].split(",")
    row_hash_col = "row_hash"
    start = begin_daily_log(table_name)
    try:
        print(f"[{_ts()}] Reading RAW and PREV_RAW for {table_name}")
        raw_df = spark.read.table(f"MHMR_LAKEHOUSE.RAW.{table_name}_RAW")
        prev_df = spark.read.table(f"MHMR_LAKEHOUSE.PREV_RAW.{table_name}_RAW_PREV")
        changes_df = build_changes(raw_df, prev_df, keys, row_hash_col, table_name)
        if changes_df is None:
            complete_daily_log(table_name, "Succeeded", start)
            return
        write_to_temp(changes_df, table_name)
        promote_to_final(table_name)
        complete_daily_log(table_name, "Succeeded", start)
    except Exception as e:
        print(f"[{_ts()}] FAILED {table_name}: {str(e)}")
        traceback.print_exc()
        complete_daily_log(table_name, "Failed", start, str(e))

def process_all():
    ensure_log_tables_exist()
    print(f"[{_ts()}] Reading TABLES_LIST...")
    tables = spark.read.table("TABLES_LIST").collect()
    print(f"[{_ts()}] Total tables: {len(tables)}")
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = [executor.submit(process_table, row.asDict()) for row in tables]
        for future in as_completed(futures):
            future.result()
    consolidate_logs_to_final()

# === RUN ===
process_all()
