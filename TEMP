# --- IMPORTS ---
try:
    print("[DEBUG] Importing modules...")
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, sha2, concat_ws, when, lit, substring
    from concurrent.futures import ThreadPoolExecutor, as_completed
    from datetime import datetime, timezone
    import pytz
    import threading
    import traceback
    import os
    import uuid
    print("[DEBUG] Imports successful.")
except Exception as e:
    print("[ERROR] Failed during imports.")
    print(traceback.format_exc())
    raise

# --- INIT SPARK ---
try:
    print("[DEBUG] Initializing Spark session...")
    spark = SparkSession.builder.appName("Process Incremental Tables").getOrCreate()
    spark.conf.set("spark.sql.session.timeZone", "UTC")
    print("[DEBUG] Spark session initialized.")
except Exception as e:
    print("[ERROR] Failed to initialize Spark session.")
    print(traceback.format_exc())
    raise

# --- CONFIG ---
try:
    print("[DEBUG] Setting configuration and paths...")
    tz = pytz.timezone("America/Chicago")
    Today_date = datetime.now(tz).date()
    print("[DEBUG] Configuration set.")
except Exception as e:
    print("[ERROR] Failed during configuration setup.")
    print(traceback.format_exc())
    raise

# --- UTILITY FUNCTIONS ---
def add_hash_column(df):
    exclude_cols = {"ROW_HASH", "insert_timestamp", "flag", "UTCTimestamp"}
    cleaned_cols = [
        when(col(field.name).isNull(), lit(" ")).otherwise(col(field.name).cast("string")).alias(field.name)
        for field in df.schema.fields if field.name not in exclude_cols
    ]
    temp_df = df.select(*cleaned_cols)
    row_hash_col = sha2(concat_ws("||", *temp_df.columns), 256).alias("ROW_HASH")
    return df.withColumn("ROW_HASH", row_hash_col)


def update_log(table_name, status, utc_timestamp_str, start_time, end_time, count=0, error_message=""):
    try:
        run_date = datetime.now(pytz.timezone("America/Chicago")).date()
        new_row = [(table_name, status, run_date, utc_timestamp_str, start_time.isoformat(), end_time.isoformat(), count, error_message)]
        schema = spark.table("final_history_logs").schema if spark._jsparkSession.catalog().tableExists("final_history_logs") else None

        new_df = spark.createDataFrame(new_row, schema=schema) if schema else spark.createDataFrame(new_row, schema=[
            col("table_name", "string"), col("status", "string"), col("load_date", "date"),
            col("UTCTimestamp", "string"), col("start_time", "string"),
            col("end_time", "string"), col("count", "int"), col("error_message", "string")
        ])

        if spark._jsparkSession.catalog().tableExists("daily_history_logs"):
            existing_df = spark.table("daily_history_logs")
            final_df = existing_df.filter(~(
                (col("table_name") == table_name) &
                (col("load_date") == run_date) &
                (col("UTCTimestamp") == utc_timestamp_str) &
                (col("status") == status)
            )).unionByName(new_df)
        else:
            final_df = new_df

        final_df.write.mode("overwrite").saveAsTable("daily_history_logs")
    except Exception as e:
        print(f"[ERROR] update_log failed for {table_name} | Status: {status}")
        print(traceback.format_exc())


def get_unprocessed_utcs(table_name):
    try:
        insert_df = spark.table(f"{table_name}_INSERT")
        update_df = spark.table(f"{table_name}_UPDATE")
        delete_df = spark.table(f"{table_name}_DELETE")

        all_utcs = set(insert_df.select("UTCTimestamp").rdd.flatMap(lambda x: x).collect()) | \
                   set(update_df.select("UTCTimestamp").rdd.flatMap(lambda x: x).collect()) | \
                   set(delete_df.select("UTCTimestamp").rdd.flatMap(lambda x: x).collect())

        if spark.catalog.tableExists("final_history_logs"):
            logged_utcs = spark.table("final_history_logs") \
                .filter((col("table_name") == table_name) & (col("status").isin("I", "U", "D"))) \
                .select("UTCTimestamp").distinct().rdd.flatMap(lambda x: x).collect()
            return sorted([utc for utc in all_utcs if utc not in set(logged_utcs)])
        else:
            return sorted(list(all_utcs))
    except Exception as e:
        print(f"[ERROR] Failed to fetch unprocessed UTCs for {table_name}.")
        print(traceback.format_exc())
        return []


def load_table(table_name, key_column, utc_list):
    start_time_total = datetime.now()
    try:
        print(f"\n====== [START] Loading table: {table_name} ======")

        insert_df = spark.table(f"{table_name}_INSERT")
        update_df = spark.table(f"{table_name}_UPDATE")
        delete_df = spark.table(f"{table_name}_DELETE")

        for utc_timestamp_str in utc_list:
            try:
                copy_table = spark.table(f"{table_name}_HIST") if spark._jsparkSession.catalog().tableExists(f"{table_name}_HIST") else None
                if copy_table is None:
                    copy_table = spark.createDataFrame([], insert_df.schema.add("flag", "string"))

                delete_view = delete_df.filter(col("UTCTimestamp") == utc_timestamp_str)
                if not delete_view.rdd.isEmpty():
                    delete_keys = delete_view.select(col("row_number").alias(key_column)).distinct()
                    delete_keys.createOrReplaceTempView("delete_keys_view")
                    spark.sql(f"""
                        UPDATE {table_name}_HIST
                        SET flag = 'I'
                        WHERE {key_column} IN (SELECT {key_column} FROM delete_keys_view)
                        AND flag = 'A'
                    """)

                insert_view = insert_df.filter(col("UTCTimestamp") == utc_timestamp_str).withColumn("flag", lit("A"))
                update_view = update_df.filter(col("UTCTimestamp") == utc_timestamp_str).withColumn("flag", lit("A"))
                merged_view = insert_view.unionByName(update_view, allowMissingColumns=True)

                if not merged_view.rdd.isEmpty():
                    merged_view = add_hash_column(merged_view)
                    merged_view.write.format("delta").mode("append").saveAsTable(f"{table_name}_HIST")

                end_time = datetime.now()
                update_log(table_name, "D", utc_timestamp_str, start_time_total, end_time, delete_view.count())
                update_log(table_name, "I", utc_timestamp_str, start_time_total, end_time, insert_view.count())
                update_log(table_name, "U", utc_timestamp_str, start_time_total, end_time, update_view.count())

            except Exception as inner_e:
                print(f"[ERROR] Exception while processing UTC {utc_timestamp_str} for table {table_name}")
                print(traceback.format_exc())
                update_log(table_name, "FAILURE", utc_timestamp_str, start_time_total, datetime.now(), 0, traceback.format_exc())

        print(f"====== [END] Loading table: {table_name} ======\n")

    except Exception as e:
        print(f"[FATAL] Failed completely while loading table {table_name}")
        print(traceback.format_exc())
        update_log(table_name, "FAILURE", str(Today_date), start_time_total, datetime.now(), 0, traceback.format_exc())


# --- MAIN EXECUTION ---
try:
    print("[DEBUG] Reading active table list...")
    table_list_df = spark.table("tables_list")
    active_tables_df = table_list_df.filter("STATUS = 'A'")
    active_tables = [(row["TABLE_NAME"], row["KEY_COLUMN"]) for row in active_tables_df.collect()]
    print(f"[DEBUG] Found {len(active_tables)} active tables.")

    print("[INFO] Starting parallel load...")
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = []
        for table_name, key_column in active_tables:
            utc_list = get_unprocessed_utcs(table_name)
            if utc_list:
                futures.append(executor.submit(load_table, table_name, key_column, utc_list))
            else:
                print(f"[SKIP] Table {table_name} has no unprocessed UTCs.")

        for future in as_completed(futures):
            print("[THREAD COMPLETE]")

    # Merge logs
    if spark._jsparkSession.catalog().tableExists("final_history_logs"):
        main_log_df = spark.table("final_history_logs")
    else:
        main_log_df = spark.createDataFrame([], schema=spark.table("daily_history_logs").schema)

    daily_log_df = spark.table("daily_history_logs")

    join_keys = ["table_name", "load_date", "UTCTimestamp", "status"]
    cleaned_main_log_df = main_log_df.alias("main").join(
        daily_log_df.select(*join_keys).alias("daily"),
        on=join_keys,
        how="left_anti"
    )
    final_main_log_df = cleaned_main_log_df.unionByName(daily_log_df)
    final_main_log_df.write.mode("overwrite").saveAsTable("final_history_logs")

    print("[COMPLETE] Final log updated and daily log reset.")
    spark.sql("DROP TABLE IF EXISTS daily_history_logs")

except Exception as e:
    print("[FATAL ERROR] Failed during execution.")
    print(traceback.format_exc())
